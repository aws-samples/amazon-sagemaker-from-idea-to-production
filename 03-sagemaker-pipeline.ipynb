{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575c15ae-ffe0-4d93-84d8-bf96278c0372",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 3: Add an ML pipeline\n",
    "\n",
    "<div class=\"alert alert-warning\"> This notebook has been last tested on a SageMaker Studio JupyterLab instance using the <code>SageMaker Distribution Image 3.6.1</code> and with the SageMaker Python SDK version <code>2.255.0</code></div>\n",
    "\n",
    "In this step you automate our end-to-end ML workflow using [Amazon SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines/) and [Amazon SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html). You make feature engineering re-usable, repeatable, and scaleable using [Amazon SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/).\n",
    "\n",
    "||||\n",
    "|---|---|---|\n",
    "|1. |Experiment in a notebook ||\n",
    "|2. |Scale with SageMaker AI processing jobs and SageMaker SDK ||\n",
    "|3. |Operationalize with ML pipeline, model registry, and feature store |**<<<< YOU ARE HERE**|\n",
    "|4. |Add a model building CI/CD pipeline ||\n",
    "|5. |Add a model deployment pipeline ||\n",
    "|6. |Add model and data monitoring ||\n",
    "\n",
    "<div class=\"alert alert-info\"> Make sure you using <code>Python 3</code> kernel in JupyterLab for this notebook.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401790be-c212-4cb1-b265-4fb0e739836a",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5Zb3UgY2FuIHNhZmVseSBpZ25vcmUgdGhlIHdhcm5pbmcgbWVzc2FnZXMuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573dc870-cf00-4083-b9d9-d274dc39d89a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To get the Feature Processor module, we need to reinstall the SageMaker python SDK along with extra dependencies\n",
    "%pip install 'sagemaker[feature-processor]<3' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb764a-efe6-4106-baf6-f2b096e14060",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --force-reinstall --no-cache s3fs boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f21890-0d4d-41e7-a64b-77fdea27d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --force-reinstall --no-cache boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd46bfb-a2b3-42d4-a136-616ddcd12be5",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5659e-ab01-4704-8063-1bdf3ce532ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca42d3f-7231-44cc-83f4-a54c6a7995f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import pathlib\n",
    "import io\n",
    "import sagemaker\n",
    "import mlflow\n",
    "from time import gmtime, strftime, sleep\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput, \n",
    "    ProcessingOutput, \n",
    "    ScriptProcessor\n",
    ")\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep, \n",
    "    TrainingStep, \n",
    "    CreateModelStep,\n",
    "    CacheConfig\n",
    ")\n",
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger, \n",
    "    ParameterFloat, \n",
    "    ParameterString, \n",
    "    ParameterBoolean\n",
    ")\n",
    "from sagemaker.workflow.quality_check_step import (\n",
    "    DataQualityCheckConfig,\n",
    "    ModelQualityCheckConfig,\n",
    "    QualityCheckStep,\n",
    ")\n",
    "from sagemaker.workflow.clarify_check_step import (\n",
    "    ModelBiasCheckConfig, \n",
    "    ClarifyCheckStep, \n",
    "    ModelExplainabilityCheckConfig\n",
    ")\n",
    "from sagemaker import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.conditions import (\n",
    "    ConditionGreaterThan,\n",
    "    ConditionGreaterThanOrEqualTo\n",
    ")\n",
    "from sagemaker.workflow.parallelism_config import ParallelismConfiguration\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import (\n",
    "    Join,\n",
    "    JsonGet\n",
    ")\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource, \n",
    "    ModelMetrics, \n",
    "    FileSource\n",
    ")\n",
    "from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig \n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.step_outputs import get_step\n",
    "from sagemaker.model_monitor import DatasetFormat, model_monitoring\n",
    "from IPython.display import HTML\n",
    "\n",
    "(sagemaker.__version__, boto3.__version__, mlflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015980fe-d2c9-44d5-aa99-08e49bc50c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ac5a8-2730-45fd-8bed-1559c422748c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r \n",
    "\n",
    "%store\n",
    "\n",
    "try:\n",
    "    initialized\n",
    "except NameError:\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] YOU HAVE TO RUN 00-start-here notebook   \")\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd6d8b-a230-4bf8-a575-2c6dcff55615",
   "metadata": {},
   "source": [
    "## Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b507e-f512-49c9-9f59-6477133bd118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set names of pipeline objects, experiment, and a model\n",
    "project = \"from-idea-to-prod\"\n",
    "\n",
    "current_timestamp = strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "registered_model_name = f\"{project}-pipeline-model-{current_timestamp}\"\n",
    "experiment_name = f\"{project}-pipeline-{current_timestamp}\"\n",
    "pipeline_name = f\"{project}-pipeline-{current_timestamp}\"\n",
    "pipeline_model_name = f\"{project}-model-xgb\"\n",
    "model_package_group_name = registered_model_name\n",
    "endpoint_config_name = f\"{project}-endpoint-config\"\n",
    "endpoint_name = f\"{project}-endpoint\"\n",
    "model_approval_status = \"PendingManualApproval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947da3b-e046-4dea-9897-1717c5bbc3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set instance types and counts\n",
    "process_instance_type = \"ml.m5.large\"\n",
    "train_instance_type = \"ml.m5.large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9be066-8452-4058-b50e-e3027cb466b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set S3 urls for various datasets produced in the pipeline\n",
    "output_s3_prefix = f\"s3://{bucket_name}/{bucket_prefix}\"\n",
    "output_s3_url = f\"{output_s3_prefix}/output\"\n",
    "\n",
    "train_s3_url = f\"{output_s3_prefix}/train\"\n",
    "validation_s3_url = f\"{output_s3_prefix}/validation\"\n",
    "test_s3_url = f\"{output_s3_prefix}/test\"\n",
    "evaluation_s3_url = f\"{output_s3_prefix}/evaluation\"\n",
    "\n",
    "baseline_s3_url = f\"{output_s3_prefix}/baseline\"\n",
    "baseline_results_s3_url = f\"{baseline_s3_url}/results\"\n",
    "\n",
    "prediction_baseline_s3_url = f\"{output_s3_prefix}/prediction_baseline\"\n",
    "prediction_baseline_results_s3_url=f\"{prediction_baseline_s3_url}/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c75128-ac7f-4d9c-a8ff-1744a6f24570",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBOOST_IMAGE_URI = sagemaker.image_uris.retrieve(\n",
    "            \"xgboost\", \n",
    "            region=boto3.Session().region_name,\n",
    "            version=\"1.7-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d649cf-b64c-4978-b3f5-cf767b7f1870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store train_s3_url\n",
    "%store validation_s3_url\n",
    "%store test_s3_url\n",
    "%store baseline_s3_url\n",
    "%store pipeline_name\n",
    "%store model_package_group_name\n",
    "%store evaluation_s3_url\n",
    "%store prediction_baseline_s3_url\n",
    "%store output_s3_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b75414-0239-4682-a435-22d646bb16cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Train S3 url: {train_s3_url}\")\n",
    "print(f\"Validation S3 url: {validation_s3_url}\")\n",
    "print(f\"Test S3 url: {test_s3_url}\")\n",
    "print(f\"Data baseline S3 url: {baseline_s3_url}\")\n",
    "print(f\"Evaluation metrics S3 url: {evaluation_s3_url}\")\n",
    "print(f\"Model prediction baseline S3 url: {prediction_baseline_s3_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1786703-1446-4cf4-9215-3903b5eb5bcf",
   "metadata": {},
   "source": [
    "## Define helper functions\n",
    "Define some shorthand functions for better code readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085266d0-0c68-4189-abd2-3031a3bd5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_estimator(\n",
    "    session,\n",
    "    instance_type,\n",
    "    output_s3_url,\n",
    "    base_job_name,\n",
    "):\n",
    "    # Instantiate an XGBoost estimator object\n",
    "    estimator = sagemaker.estimator.Estimator(\n",
    "        image_uri=XGBOOST_IMAGE_URI,\n",
    "        role=sagemaker.get_execution_role(), \n",
    "        instance_type=instance_type,\n",
    "        instance_count=1,\n",
    "        output_path=output_s3_url,\n",
    "        sagemaker_session=session,\n",
    "        base_job_name=base_job_name,\n",
    "    )\n",
    "    \n",
    "    # Define algorithm hyperparameters\n",
    "    estimator.set_hyperparameters(\n",
    "        num_round=100, # the number of rounds to run the training\n",
    "        max_depth=3, # maximum depth of a tree\n",
    "        eta=0.5, # step size shrinkage used in updates to prevent overfitting\n",
    "        alpha=2.5, # L1 regularization term on weights\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\", # evaluation metrics for validation data\n",
    "        subsample=0.8, # subsample ratio of the training instance\n",
    "        colsample_bytree=0.8, # subsample ratio of columns when constructing each tree\n",
    "        min_child_weight=3, # minimum sum of instance weight (hessian) needed in a child\n",
    "        early_stopping_rounds=10, # the model trains until the validation score stops improving\n",
    "        verbosity=1, # verbosity of printing messages\n",
    "    )\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195eda2d-0a32-42e5-b296-ed9dee23bdd1",
   "metadata": {},
   "source": [
    "## Configure MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef5b2c-a694-476f-9c94-bb5204a3768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the MLflow server is in the status 'Created' or 'Started'\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "while sm.describe_mlflow_tracking_server(TrackingServerName=mlflow_name)['TrackingServerStatus'] not in ['Created', 'Started']:\n",
    "    print(f\"The MLflow server {mlflow_name} is not in the status 'Created' or 'Started'\")\n",
    "    sleep(30)\n",
    "else:\n",
    "    print(f\"Using server {mlflow_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7654f0c-c026-42db-8b3b-cddf2283a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(mlflow_arn)\n",
    "experiment = mlflow.set_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab04e2c-76cc-4f42-9629-0ccf6e47ed13",
   "metadata": {},
   "source": [
    "## Configure defaults for AWS infrastructure\n",
    "You can use YAML configuration file to define the default values that are automatically passed to SageMaker APIs, for example as job parameters. It's especially convenient when you need to provide static parameters for infrastructure settings, such as VPC ids, Security Groups, KMS keys etc, or work with remote functions.\n",
    "\n",
    "Refer to [Configuring and using defaults with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk) documentation for examples and more details.\n",
    "\n",
    "Your SageMaker pipeline will use these `config.yaml` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10fbf3-55e1-4268-9d61-d936f4174d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print default location of configuration files\n",
    "import os\n",
    "from platformdirs import site_config_dir, user_config_dir\n",
    "\n",
    "#Prints the location of the admin config file\n",
    "print(os.path.join(site_config_dir(\"sagemaker\"), \"config.yaml\"))\n",
    "\n",
    "#Prints the location of the user config file\n",
    "print(os.path.join(user_config_dir(\"sagemaker\"), \"config.yaml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed9188-9c05-4ac8-aaae-da7f4fda3952",
   "metadata": {},
   "source": [
    "The next cell creates a configuration file and sets default values for remote functions. These values are automatically passed to `@step` decorator and you don't need to specify them explicitly. Refer to [Configure your pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-step-decorator-cfg-pipeline.html) in the Developer Guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33320fa-f008-48d0-adbe-82af13a54708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "SchemaVersion: '1.0'\n",
    "SageMaker:\n",
    "    PythonSDK:\n",
    "        Modules:\n",
    "            RemoteFunction:\n",
    "                InstanceType: ml.m5.xlarge\n",
    "                Dependencies: ./requirements.txt\n",
    "                IncludeLocalWorkDir: true\n",
    "                CustomFileFilter:\n",
    "                    IgnoreNamePatterns: # files or directories to ignore\n",
    "                        - \"*.ipynb\" # all notebook files\n",
    "                        - \"*.md\" # all markdown files\n",
    "                        - \"__pycache__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41475d62-58e2-4f74-8bbc-2d9a6ece2ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy the configuration file to user config file location\n",
    "%mkdir -p {user_config_dir(\"sagemaker\")}\n",
    "%cp config.yaml {os.path.join(user_config_dir(\"sagemaker\"), \"config.yaml\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e343f69-4076-454e-81c7-902af1821054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively instead of copying config.yaml to user config dir, you can point SageMaker to the configuration file\n",
    "# os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eac4cf-4875-453f-a788-f7f52c94a37b",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8a63a-2c4f-4cbd-9de6-4550b5710817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install xgboost for the local testing of the code\n",
    "%pip install -q xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064409a8-a5e3-4d65-86a3-c7822e2d0307",
   "metadata": {},
   "source": [
    "Get the version of the installed packages and create a `requirements.txt` file to replicate the environment. Your SageMaker pipeline is going to use it to setup environment in job containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6d823-f122-421b-9ef4-6b72ccfd9050",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('requirements.txt'):\n",
    "    os.remove('requirements.txt')\n",
    "    print(\"Existing requirements.txt file deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273f636-ee36-478f-b7c4-bd017b2af4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = ['xgboost', 'mlflow', 'sagemaker-mlflow','sagemaker']\n",
    "requirements = [f'{p}=={version(p)}' for p in packages]\n",
    "\n",
    "requirements.append('protobuf==3.20.1')\n",
    "requirements.append('s3fs==0.4.2') #FIXME: find a more up to date version that works\n",
    "\n",
    "if requirements:\n",
    "    with open('requirements.txt', 'w') as f:\n",
    "        f.write('\\n'.join(requirements))\n",
    "    print(\"\\nNew requirements.txt file created with the following content:\")\n",
    "    print('\\n'.join(requirements))\n",
    "else:\n",
    "    print(\"\\nNo requirements.txt file created as no packages were found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b416e96-b3be-43f0-a434-42c05fb1e63b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile requirements.txt\n",
    "# scikit-learn\n",
    "# pandas>=2.0.0\n",
    "# s3fs==0.4.2\n",
    "# sagemaker>=2.237\n",
    "# xgboost\n",
    "# mlflow==2.16.2\n",
    "# sagemaker-mlflow==0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ea883-3bd9-4947-a0c8-7c3a80c1bbca",
   "metadata": {},
   "source": [
    "## A SageMaker pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00ef9a-a6fe-4415-9847-bcfa97f32c44",
   "metadata": {},
   "source": [
    "### Setup pipeline parameters\n",
    "SageMaker Pipelines supports [parameterization](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html), which allows you to specify input parameters at runtime without changing your pipeline code. You can use the parameter classes available under the [`sagemaker.workflow.parameters`](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#parameters) module.\n",
    "Parameters have a default value, which you can override by specifying parameter values when starting a pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f370b6-1ee3-4bc6-ae85-5eef37878196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set processing instance type\n",
    "process_instance_type_param = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=process_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance type\n",
    "train_instance_type_param = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=train_instance_type,\n",
    ")\n",
    "\n",
    "# Set model approval status for the model registry\n",
    "model_approval_status_param = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=model_approval_status\n",
    ")\n",
    "\n",
    "#Â Minimal threshold for model performance on the test dataset\n",
    "test_score_threshold_param = ParameterFloat(\n",
    "    name=\"TestScoreThreshold\",\n",
    "    default_value=0.75\n",
    ")\n",
    "\n",
    "# Parametrize the S3 url for input dataset\n",
    "input_s3_url_param = ParameterString(\n",
    "    name=\"InputDataUrl\",\n",
    "    default_value=input_s3_url,\n",
    ")\n",
    "\n",
    "# Model package group name\n",
    "model_package_group_name_param = ParameterString(\n",
    "    name=\"ModelPackageGroupName\",\n",
    "    default_value=model_package_group_name,\n",
    ")\n",
    "\n",
    "# MLflow tracking server ARN\n",
    "tracking_server_arn_param = ParameterString(\n",
    "    name=\"TrackingServerARN\",\n",
    "    default_value=mlflow_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a71d7d-f040-498b-b838-e65a88102b5b",
   "metadata": {},
   "source": [
    "### Implement and test the pipeline steps\n",
    "You create a pipeline with the following:\n",
    "| Step | Description |\n",
    "|---|---|\n",
    "| **Data processing** | runs a SageMaker processing job for feature engineering and dataset split|\n",
    "| **Training** | runs a SageMaker training job using XGBoost algorithm |\n",
    "| **Evaluation** | evaluates the performance of the trained model |\n",
    "| **Conditional step** | checks if the performance of the model meets the specified threshold |\n",
    "| **Register model** | registers a version of the model in the SageMaker model registry |\n",
    "\n",
    "To facilitate implementation of your pipeline as code you're going to use two useful features of SageMaker: Subclass compatibility and @step decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6ec12-39b0-407d-abc2-63ecdb896969",
   "metadata": {},
   "source": [
    "#### Subclass compatibility\n",
    "ðŸ’¡ You can use subclass compatibility for [workflow pipeline job steps](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#steps) to build job abstractions and use exactly the same code to configure the pipeline as the code for running processing, training, transform, and tuning jobs from the previous step notebooks. You need to use [PipelineSession](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline_context.PipelineSession) context instead of `sagemaker_session` to capture the run calls such as `processor.run()` or `estimator.fit()` but not run until the pipeline is created and executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f78d11-5546-4267-921e-017910888b9d",
   "metadata": {},
   "source": [
    "#### @step decorator\n",
    "ðŸ’¡ You can lift-and-shift your existing Python code to SageMaker pipelines. You can also use Python functions to implement an ML workflow using SageMaker Python SDK and test all code locally in the notebook. When you want to create a pipeline, you can use the SageMaker Python SDK [`@step`](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#step-decorator) decorator to convert Python functions into pipeline steps. Refer to the SageMaker [Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-step-decorator-create-pipeline.html) for more details and examples.\n",
    "\n",
    "The following code uses Python functions to implement workflow steps, test them locally, and then apply `@step` decorator to re-use the function as a pipeline step.\n",
    "\n",
    "#### Limitations with @step decorator\n",
    "Be aware of the specific [limitations](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-step-decorator-limit.html) when you use `@step` decorator for pipeline steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6dcca-fa05-459b-8293-dfb42bd637dc",
   "metadata": {},
   "source": [
    "Start developing and testing steps in the following code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10b118-2985-41a9-b1af-da7111060f68",
   "metadata": {},
   "source": [
    "#### Processing step\n",
    "Re-use the Python function local code from the step 2 [notebook](./02-sagemaker-containers.ipynb) and create a `preprocess` function in the `./pipeline_steps` folder. You're going to use this function to create a pipeline processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4541fa1-50b6-4664-9239-b4d78b46f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function code is in the local files\n",
    "from pipeline_steps.preprocess import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8b08b-7078-453d-8e43-218123e9752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the function code\n",
    "# !pygmentize pipeline_steps/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b04a6-63fa-405e-adfd-de67a25a964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there is dataset under S3 url\n",
    "!aws s3 ls {input_s3_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c62018-cfcb-453f-aa25-608d639ef713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run your Python code locally and verify the corectness before constructing a pipeline\n",
    "r_preprocess = preprocess(\n",
    "    input_data_s3_path=input_s3_url,\n",
    "    output_s3_prefix=output_s3_prefix,\n",
    "    tracking_server_arn=mlflow_arn,\n",
    "    experiment_name=f\"local-test-{current_timestamp}\"\n",
    ")\n",
    "r_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a83107-2c15-476e-bce9-a25409c9d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the function generated output\n",
    "!aws s3 ls {output_s3_prefix}/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21faa341-c920-47fc-bf79-1453f7982f95",
   "metadata": {},
   "source": [
    "#### Training step\n",
    "First, run a model training remotely as a SageMaker built-in algorithm training job. \n",
    "\n",
    "Second, use the trained model to test the evaluation script as a local Python function.\n",
    "\n",
    "Third, use this Python function to construct an evaluation step in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4707d-7d82-41c4-9e9a-1dcf4e349b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sagemaker.Session() in the estimator to run a training job immediately\n",
    "estimator = get_xgb_estimator(\n",
    "    session=sagemaker.Session(),\n",
    "    instance_type=train_instance_type,\n",
    "    output_s3_url=output_s3_url,\n",
    "    base_job_name=f\"{project}-train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa66a21-2c07-41a1-b1c6-ea5224331949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training inputs using the outputs from preprocess function\n",
    "training_inputs = {\n",
    "    \"train\": TrainingInput(\n",
    "        s3_data=r_preprocess['train_data'],\n",
    "        content_type=\"text/csv\",\n",
    "    ),\n",
    "    \"validation\": TrainingInput(\n",
    "        s3_data=r_preprocess['validation_data'],\n",
    "        content_type=\"text/csv\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ca889-3f9e-44e7-b6b8-9a380cfcfa19",
   "metadata": {},
   "source": [
    "The next code cell fits the estimator. Wait less than 3 minutes until the training job finishes. Note that the code also logs the trained model into MLflow experiment run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e54a4b-f0c6-4fdb-98b7-9f7d6719d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_steps.evaluate import load_model\n",
    "\n",
    "# Run the training job\n",
    "mlflow.set_experiment(r_preprocess['experiment_name'])\n",
    "with mlflow.start_run(\n",
    "    run_name=f\"training-{strftime('%d-%H-%M-%S', gmtime())}\",\n",
    "    description=\"training in the notebook 03 with a training job\") as run:\n",
    "    mlflow.log_params(estimator.hyperparameters())\n",
    "    \n",
    "    estimator.fit(training_inputs)\n",
    "\n",
    "    mlflow.log_param(\"training job name\", estimator.latest_training_job.name)\n",
    "    mlflow.log_metrics({i['metric_name'].replace(':', '_'):i['value'] for i in estimator.training_job_analytics.dataframe().iloc})\n",
    "    mlflow.xgboost.log_model(load_model(estimator.model_data), artifact_path=\"xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f365c-94c4-4403-8a8c-b79399b3d665",
   "metadata": {},
   "source": [
    "#### Evaluation step\n",
    "Create a model evaluation script as a local Python function to check if the model performance meets the specified threshold. The Python code is in the file `evaluate.py` in the `./pipeline_steps` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5075d-3bf8-48c7-abdf-32c023120064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_steps.evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36e8dc-c04b-4273-9fd5-b26f7c0e0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize  pipeline_steps/evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d627e-7718-45a9-a90e-b6bba4b5fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the training job you've just run created a model file\n",
    "!aws s3 ls {estimator.model_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce46e1-2e13-46ae-9e6c-8a2b8f77b0fa",
   "metadata": {},
   "source": [
    "Now load the trained model in the evaluation script and verify that the script executes correctly. Note the usage of output parameters from the `preprocess` function and the model artifact from the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb9f0f-a475-4dab-8d9c-dca4220978e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation code locally\n",
    "r_eval = evaluate(\n",
    "    test_x_data_s3_path=r_preprocess['test_x_data'],\n",
    "    test_y_data_s3_path=r_preprocess['test_y_data'],\n",
    "    model_s3_path=estimator.model_data,\n",
    "    output_s3_prefix=output_s3_prefix,\n",
    "    tracking_server_arn=mlflow_arn,\n",
    "    experiment_name=r_preprocess['experiment_name'],\n",
    ")\n",
    "r_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee22d5-a9ce-4c2e-b9f5-79c191c50d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the evaluation function generated output\n",
    "!aws s3 ls {output_s3_prefix}/prediction_baseline/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a5348-9c06-416a-a4b2-0c83e4ea34f4",
   "metadata": {},
   "source": [
    "#### Model registration step\n",
    "The register step creates a SageMaker model and registers a new version of a model in the SageMaker Model Registry within a [model package group](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-model-group.html). \n",
    "You implement this step also as a local Python function. The code is provided in the file `register.py` in the `./pipeline_steps` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06623ea6-4657-4fdd-913e-0f1bdd80c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_steps.register import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6908ae-16ec-4228-8d9d-2b5f4290c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize  pipeline_steps/register.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421dd3d8-8a0a-46f1-bb59-f4a95e55a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model register code locally\n",
    "r_register = register(\n",
    "    training_job_name=estimator.latest_training_job.name,\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_approval_status=model_approval_status,\n",
    "    evaluation_result=r_eval['evaluation_result'],\n",
    "    output_s3_prefix=output_s3_url,\n",
    "    tracking_server_arn=mlflow_arn,\n",
    "    experiment_name=r_preprocess['experiment_name'],\n",
    ")\n",
    "r_register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afbcb0-a145-4662-bcfb-76cd7892c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that a new model version has been registered in the model package group\n",
    "boto3.client('sagemaker').describe_model_package(ModelPackageName=r_register['model_package_arn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d66c4f-f26b-4061-ae60-20308990f373",
   "metadata": {},
   "source": [
    "### Construct a pipeline\n",
    "After local testing you can use the same Python code without any changes to construct a pipeline.\n",
    "\n",
    "The next cell creates a pipeline with previously developed and tested steps. Note, that you mix `@step`-decorated functions (preprocess, evaluate, register) and traditional pipeline steps (train) as SageMaker jobs in the same pipeline and pass data between them.\n",
    "\n",
    "You don't need to manually define an ordering of the steps, as SageMaker automatically derives the processing flow based on data dependencies between pipeline's steps. You also don't need to manage transfer of artifacts and datasets from one pipeline's step to another, because SageMaker automatically takes care of the data flow.\n",
    "\n",
    "Note the use of [`PipelineSession`](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline_context.PipelineSession) instead of Session in the estimator object for the training step. When constructing a pipeline, a PipelineSession object must be given to the constructor of `Estimator` or `Processor` to start the job at pipeline execution time and not immedeately as with SageMaker `Session`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c163656-a447-4a90-9af1-63c54f07a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data step\n",
    "step_preprocess = step(\n",
    "    preprocess, \n",
    "    instance_type=process_instance_type_param,\n",
    "    name=f\"{project}-preprocess\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    ")(\n",
    "    input_data_s3_path=input_s3_url_param,\n",
    "    output_s3_prefix=output_s3_prefix,\n",
    "    tracking_server_arn=tracking_server_arn_param,\n",
    "    experiment_name=experiment_name,\n",
    "    pipeline_run_name=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    ")\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True)\n",
    "cache_config.expire_after = \"p30d\"\n",
    "\n",
    "# train step\n",
    "step_train = TrainingStep(\n",
    "    name=f\"{project}-train\",\n",
    "    step_args=get_xgb_estimator(\n",
    "        session=PipelineSession(),\n",
    "        instance_type=train_instance_type_param,\n",
    "        output_s3_url=output_s3_url,\n",
    "        base_job_name=f\"{project}-train\",\n",
    "    ).fit(\n",
    "        {\n",
    "            \"train\": TrainingInput(\n",
    "                step_preprocess['train_data'],\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                step_preprocess['validation_data'],\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "        }\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")    \n",
    "\n",
    "# evaluate step\n",
    "step_evaluate = step(\n",
    "    evaluate,\n",
    "    instance_type=process_instance_type_param,\n",
    "    name=f\"{project}-evaluate\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    ")(\n",
    "    test_x_data_s3_path=step_preprocess['test_x_data'],\n",
    "    test_y_data_s3_path=step_preprocess['test_y_data'],\n",
    "    model_s3_path=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    output_s3_prefix=output_s3_prefix,\n",
    "    tracking_server_arn=tracking_server_arn_param,\n",
    "    experiment_name=step_preprocess['experiment_name'],\n",
    "    pipeline_run_id=step_preprocess['pipeline_run_id'],\n",
    ")\n",
    "\n",
    "# register model step\n",
    "step_register = step(\n",
    "        register,\n",
    "        name=f\"{project}-register\",\n",
    "        keep_alive_period_in_seconds=3600,\n",
    "    )(\n",
    "        training_job_name=step_train.properties.TrainingJobName,\n",
    "        model_package_group_name=model_package_group_name_param,\n",
    "        model_approval_status=model_approval_status_param,\n",
    "        evaluation_result=step_evaluate['evaluation_result'],\n",
    "        output_s3_prefix=output_s3_url,\n",
    "        tracking_server_arn=tracking_server_arn_param,\n",
    "        experiment_name=step_preprocess['experiment_name'],\n",
    "        pipeline_run_id=step_preprocess['pipeline_run_id'],\n",
    "    )\n",
    "\n",
    "# fail the pipeline execution step\n",
    "step_fail = FailStep(\n",
    "    name=f\"{project}-fail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to AUC Score < \", test_score_threshold_param]),\n",
    ")\n",
    "\n",
    "# condition to check in the condition step\n",
    "condition_gte = ConditionGreaterThanOrEqualTo(\n",
    "        left=step_evaluate['evaluation_result']['classification_metrics']['auc_score']['value'],  \n",
    "        right=test_score_threshold_param,\n",
    ")\n",
    "\n",
    "# conditional register step\n",
    "step_conditional_register = ConditionStep(\n",
    "    name=f\"{project}-check-metrics\",\n",
    "    conditions=[condition_gte],\n",
    "    if_steps=[step_register],\n",
    "    else_steps=[step_fail],\n",
    ")\n",
    "\n",
    "# Create a pipeline object\n",
    "pipeline = Pipeline(\n",
    "    name=f\"{pipeline_name}\",\n",
    "    parameters=[\n",
    "        input_s3_url_param,\n",
    "        process_instance_type_param,\n",
    "        train_instance_type_param,\n",
    "        model_approval_status_param,\n",
    "        test_score_threshold_param,\n",
    "        model_package_group_name_param,\n",
    "        tracking_server_arn_param,\n",
    "    ],\n",
    "    steps=[step_conditional_register],\n",
    "    pipeline_definition_config=PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f8588-bbda-4501-83e3-dee157ff48ee",
   "metadata": {},
   "source": [
    "Note, we added two more steps to the pipeline: Fail step and Condition Step.\n",
    "\n",
    "#### Fail step\n",
    "A Pipelines [FailStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.fail_step.FailStep) stops the pipeline execution if the model performance metric doesn't meet the specified threshold. \n",
    "\n",
    "#### Condition step\n",
    "The condition step checks the model performance score calculated in the evaluation step and conditionally creates a model and registers it in the model registry, or stops and fails the pipeline execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce14bd92-a3e6-45db-a1c5-8f476038fae8",
   "metadata": {},
   "source": [
    "#### Upsert the pipeline\n",
    "Now create the pipeline. If a pipeline with the same name already exits, SageMaker will update it. \n",
    "\n",
    "You need to pass only the last step to `Pipeline` constructor. The SDK automatically builds a pipeline DAG based on data dependencies between steps. Refer to the [Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-step-decorator-create-pipeline.html#pipelines-step-define-delayed) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea591a-a789-4684-9262-b544ab9294bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert operation serialize the function code, arguments, and other artefacts to S3 where it can be accessed during pipeline's runtime\n",
    "pipeline.upsert(role_arn=sm_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a312dae-75ba-4dbb-9266-cf68c11c681f",
   "metadata": {},
   "source": [
    "To see the created pipeline in the Studio UI, click on the link constructed by the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94b5fe-f56c-46f2-a4f4-643af4bab979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the pipeline link\n",
    "display(\n",
    "    HTML('<b>See <a target=\"top\" href=\"https://studio-{}.studio.{}.sagemaker.aws/pipelines/{}/graph\">the pipeline</a> in the Studio UI</b>'.format(\n",
    "            domain_id, region, pipeline_name))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3608ce-fcab-4e89-be8f-07a060675e14",
   "metadata": {
    "tags": []
   },
   "source": [
    "Based on the data dependencies between the pipeline's steps, SageMaker builds the following DAG with the data flow in your pipeline:\n",
    "![](img/pipeline-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d83cd-72c0-403c-b9f1-e953f91add9d",
   "metadata": {},
   "source": [
    "### Execute the pipeline\n",
    "The first pipeline execution takes about 17-20 minutes. Note the usage of the `keep_alive_period_in_seconds` parameter in the step definition for the warm pool reuse and `CacheConfig` in the Training step for the caching of step results.\n",
    "A subsequent pipeline execution takes about 7 minutes due to usage of caching and a warm pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b84443-b4f4-4f89-8fbb-e813e9b9997b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_execution = pipeline.start()\n",
    "pipeline_execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c501c9-02ba-4910-a42f-1a2398b401e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment if you would like to wait in the notebook until this execution completes\n",
    "# pipeline_execution.wait() \n",
    "pipeline_execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25826e-e7cf-4e27-8662-408bc682a9d6",
   "metadata": {},
   "source": [
    "You can see the pipeline execution in the Studio UI by clicking on the link constructed by the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8c325-7ddd-4c5a-91a7-c6ca7c40d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the pipeline execution link\n",
    "display(\n",
    "    HTML('<b>See <a target=\"top\" href=\"https://studio-{}.studio.{}.sagemaker.aws/pipelines/{}/executions/{}/graph\">the pipeline execution</a> in the Studio UI</b>'.format(\n",
    "            domain_id, region, pipeline_name, pipeline_execution.describe()['PipelineExecutionArn'].split('/')[-1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70ee2a-a099-467a-b13c-85fc567f1e4f",
   "metadata": {},
   "source": [
    "To manage pipelines in the Studio UI select **Pipelines** in the navigation menu on the left and then the specific pipeline to see pipeline's executions and all details:\n",
    "\n",
    "![](img/pipelines-pane.png)\n",
    "\n",
    "For each execution you can open an execution graph and see details of each pipeline step:\n",
    "\n",
    "![](img/pipeline-execution-graph.png)\n",
    "\n",
    "You can track pipeline executions, artifacts, datasets, and models in the MLflow UX:\n",
    "\n",
    "![](img/mlflow-pipeline-executions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a32911-b094-450f-8d4a-450c6448290f",
   "metadata": {},
   "source": [
    "### Understand the pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d05813-b615-463b-9b90-67f45c9444cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_definition = json.loads(pipeline.describe()['PipelineDefinition'])\n",
    "pipeline_definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297037d-a04c-463a-9353-c362fcb3677d",
   "metadata": {},
   "source": [
    "Look at and understand the pipeline definition JSON. For example, you can see, how the pipeline paramemters are defined and how are they used.\n",
    "\n",
    "Definition:\n",
    "```json\n",
    "'Parameters': [{'Name': 'ProcessingInstanceType',\n",
    "   'Type': 'String',\n",
    "   'DefaultValue': 'ml.m5.large'},\n",
    "               ...\n",
    "               ]\n",
    "```\n",
    "\n",
    "Parameter substitution:\n",
    "```json\n",
    "'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType':{'Get':'Parameters.ProcessingInstanceType'}\n",
    "```\n",
    "\n",
    "Now find the definition of `preprocess` step:\n",
    "\n",
    "```json\n",
    "{'Name': 'from-idea-to-prod-preprocess',\n",
    "   'Type': 'Training',\n",
    "   'Arguments': {'TrainingJobName': 'preprocess',\n",
    "    'RoleArn': 'arn:aws:iam::906545278380:role/service-role/AmazonSageMaker-ExecutionRole-20240214T222844',\n",
    "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
    "    'RetryStrategy': {'MaximumRetryAttempts': 1},\n",
    "    'InputDataConfig': [{'ChannelName': 'sagemaker_remote_function_bootstrap',\n",
    "      'DataSource': {'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-1-906545278380/from-idea-to-prod-pipeline-03-09-21-09/sagemaker_remote_function_bootstrap',\n",
    "        'S3DataType': 'S3Prefix'}}},\n",
    "```\n",
    "\n",
    "You see that SageMaker by default runs your Python script in a container with the same image as the kernel image used for this JupyterLab notebook:\n",
    "\n",
    "```json\n",
    "'AlgorithmSpecification': {'TrainingImage': '885854791233.dkr.ecr.us-east-1.amazonaws.com/sagemaker-distribution-prod@sha256:7c07530831d3d25b27a77b6a77f9801eec01b7b80c69ca1aa2c9eae3df00887d',\n",
    "```\n",
    "\n",
    "If you want to create pipelines by hand, you can work with JSON and follow the [SageMaker Pipeline Definition JSON Schema](https://aws-sagemaker-mlops.github.io/sagemaker-model-building-pipeline-definition-JSON-schema/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc25fb32-eaab-4b01-8ccc-a79908afce80",
   "metadata": {},
   "source": [
    "## Add a batch transform and quality checks to the pipeline\n",
    "You can integrate additional steps in your model building pipeline to automate all required tasks. In this section you're going to add the following steps:\n",
    "- Quality checks for both data and the model and baseline calculation using [`QualityCheckStep`](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-quality-check)\n",
    "- Batch transform using [`TransformStep`](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-transform)\n",
    "\n",
    "For a more detailed example with model quality checks refer to an example notebook [SageMaker Pipelines integration with Model Monitor and Clarify](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/model-monitor-clarify-pipelines/sagemaker-pipeline-model-monitor-clarify-steps.ipynb).\n",
    "\n",
    "\n",
    "To understand the data and model quality life cycle refer to the Developer Guide [Baseline calculation, drift detection and lifecycle with ClarifyCheck and QualityCheck steps in Amazon SageMaker Model Building Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-quality-clarify-baseline-lifecycle.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848224f-40be-4b84-ba11-78505227444b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.inputs import CreateModelInput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156045d-8785-4e39-9e7a-42d86a2a9198",
   "metadata": {},
   "source": [
    "###Â Quality checks\n",
    "Start with definition of data and model quality check steps for the pipeline. The data and model quality check steps use data from `preprocess` and `evaluate` steps of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298f32c-506f-4902-a392-f125d4545d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to control data quality check\n",
    "skip_check_data_quality_param = ParameterBoolean(name=\"SkipDataQualityCheck\", default_value=True)\n",
    "register_new_baseline_data_quality_param = ParameterBoolean(\n",
    "    name=\"RegisterNewDataQualityBaseline\", default_value=True\n",
    ")\n",
    "\n",
    "# Parameters to control model quality check\n",
    "skip_check_model_quality_param = ParameterBoolean(name=\"SkipModelQualityCheck\", default_value=True)\n",
    "register_new_baseline_model_quality_param = ParameterBoolean(\n",
    "    name=\"RegisterNewModelQualityBaseline\", default_value=True\n",
    ")\n",
    "\n",
    "# Job configuration for both data and model quality check steps\n",
    "check_job_config = CheckJobConfig(\n",
    "    role=sm_role,\n",
    "    instance_count=1,\n",
    "    instance_type=process_instance_type_param,\n",
    ")\n",
    "\n",
    "# Configuration for data quality check step\n",
    "data_quality_check_config = DataQualityCheckConfig(\n",
    "    baseline_dataset=step_preprocess['baseline_data'],\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=baseline_results_s3_url,\n",
    ")\n",
    "\n",
    "# Configuration for model quality check step\n",
    "model_quality_check_config = ModelQualityCheckConfig(\n",
    "    baseline_dataset=step_evaluate['prediction_baseline_data'],\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=prediction_baseline_results_s3_url,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    inference_attribute= \"prediction\", # The column in the dataset that contains predictions\n",
    "    probability_attribute= \"probability\", # The column in the dataset that contains probabilities\n",
    "    ground_truth_attribute= \"label\", # The column in the dataset that contains ground truth labels\n",
    ")\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True)\n",
    "cache_config.expire_after = \"p30d\"\n",
    "\n",
    "# Data quality check step\n",
    "step_data_quality_check = QualityCheckStep(\n",
    "    name=f\"{project}-data-quality\",\n",
    "    quality_check_config=data_quality_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_data_quality_param,\n",
    "    register_new_baseline=register_new_baseline_data_quality_param,\n",
    "    model_package_group_name=model_package_group_name_param,\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n",
    "# Model quality check step\n",
    "step_model_quality_check = QualityCheckStep(\n",
    "    name=f\"{project}-model-quality\",\n",
    "    quality_check_config=model_quality_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_model_quality_param,\n",
    "    register_new_baseline=register_new_baseline_model_quality_param,\n",
    "    model_package_group_name=model_package_group_name_param,\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f800e-50b9-4406-8441-d30bcc6f2f0a",
   "metadata": {},
   "source": [
    "### Batch transform\n",
    "Add a transform step to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1445b-af06-404a-b824-634553aa9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = PipelineSession()\n",
    "\n",
    "#Â create model step\n",
    "step_create_model = ModelStep(\n",
    "    name=f\"{project}-model\",\n",
    "    step_args=Model(\n",
    "        image_uri=XGBOOST_IMAGE_URI,        \n",
    "        model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        name=f\"from-idea-to-prod-xgboost-model\",\n",
    "        sagemaker_session=session,\n",
    "        role=sm_role,\n",
    "    ).create(instance_type=\"ml.m5.large\"),\n",
    ")\n",
    "\n",
    "# create the transform step\n",
    "step_transform = TransformStep(\n",
    "    name=f\"{project}-transform\", \n",
    "    step_args=Transformer(\n",
    "        model_name=step_create_model.properties.ModelName,\n",
    "        instance_type=train_instance_type_param,\n",
    "        instance_count=1,\n",
    "        accept=\"text/csv\",\n",
    "        assemble_with=\"Line\",\n",
    "        output_path=f\"{output_s3_prefix}/transform\",\n",
    "        sagemaker_session=session,\n",
    "        base_transform_job_name=f\"{project}-transform\",\n",
    "    ).transform(    \n",
    "        data=step_preprocess[\"test_x_data\"],\n",
    "        content_type=\"text/csv\",\n",
    "        split_type=\"Line\", \n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508a184-33f4-4e5e-ac88-a78aa81de30a",
   "metadata": {},
   "source": [
    "You need to include the generated model and data quality baselines into the model register step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c504c0c-6ead-4ede-b1d9-902a90509e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-define the register model step to use the calculated model and data quality baselines\n",
    "step_register = step(\n",
    "        register,\n",
    "        name=f\"{project}-register\",\n",
    "        keep_alive_period_in_seconds=3600,\n",
    "    )(\n",
    "        training_job_name=step_train.properties.TrainingJobName,\n",
    "        model_package_group_name=model_package_group_name_param,\n",
    "        model_approval_status=model_approval_status_param,\n",
    "        evaluation_result=step_evaluate['evaluation_result'],\n",
    "        output_s3_prefix=output_s3_url,\n",
    "        tracking_server_arn=tracking_server_arn_param,\n",
    "        model_statistics_s3_path=step_model_quality_check.properties.CalculatedBaselineStatistics,\n",
    "        model_constraints_s3_path=step_model_quality_check.properties.CalculatedBaselineConstraints,\n",
    "        model_data_statistics_s3_path=step_data_quality_check.properties.CalculatedBaselineStatistics,\n",
    "        model_data_constraints_s3_path=step_data_quality_check.properties.CalculatedBaselineConstraints,\n",
    "        experiment_name=step_preprocess['experiment_name'],\n",
    "        pipeline_run_id=step_preprocess['pipeline_run_id'],\n",
    "    )\n",
    "\n",
    "# re-define the conditional register step with the new step_register\n",
    "step_conditional_register = ConditionStep(\n",
    "    name=f\"{project}-check-metrics\",\n",
    "    conditions=[condition_gte],\n",
    "    if_steps=[step_register, step_transform],\n",
    "    else_steps=[step_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859e755-b0a6-4d26-8cb8-e7dd6c86755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline object\n",
    "pipeline = Pipeline(\n",
    "    name=f\"{pipeline_name}\",\n",
    "    parameters=[\n",
    "        input_s3_url_param,\n",
    "        process_instance_type_param,\n",
    "        train_instance_type_param,\n",
    "        model_approval_status_param,\n",
    "        test_score_threshold_param,\n",
    "        model_package_group_name_param,\n",
    "        tracking_server_arn_param,\n",
    "        skip_check_data_quality_param,\n",
    "        skip_check_model_quality_param,\n",
    "        register_new_baseline_data_quality_param,\n",
    "        register_new_baseline_model_quality_param,\n",
    "    ],\n",
    "    steps=[step_conditional_register],\n",
    "    pipeline_definition_config=PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05870f01-a36b-4ae1-b3d8-be0df7f4c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the pipeline\n",
    "pipeline.upsert(role_arn=sm_role, parallelism_config=ParallelismConfiguration(5).to_request())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d3f73-6293-4d87-a474-be9d3738150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the pipeline link\n",
    "display(\n",
    "    HTML('<b>See <a target=\"top\" href=\"https://studio-{}.studio.{}.sagemaker.aws/pipelines/{}/graph\">the pipeline</a> in the Studio UI</b>'.format(\n",
    "            domain_id, region, pipeline_name))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6dfdbc-af2f-4555-8750-6f4c36e0a981",
   "metadata": {},
   "source": [
    "The new pipeline contains now the steps for data and model quality checks and the batch transform:\n",
    "![](img/pipeline-graph-with-transform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea41f01-21f6-4154-8fb4-42bf3f0d6524",
   "metadata": {},
   "source": [
    "### Execute the new pipeline\n",
    "The first time the pipeline runs the parameters `skip_check_...` and `register_new_baseline_...` need to be set to default values `(True, True)` so that the quality checks are skipped and newly calculated baselines are registered for the model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82845760-890e-4db2-9a21-af99f9134a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution = pipeline.start()\n",
    "pipeline_execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75aa4f-9199-47fd-9f6d-239b2e85139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you would like to wait in the notebook until this execution completes\n",
    "# pipeline_execution.wait() \n",
    "pipeline_execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49118c7-f123-4983-a12f-c38060e27669",
   "metadata": {},
   "source": [
    "If you'd like to execute the pipeline one more time, you can set the `SkipDataQualityCheck` parameter to `True` to run the data quality check by comparing the generated baseline with the input dataset and model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55436e93-6602-409a-8cfb-d833892c11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you would like to start one more execution\n",
    "# pipeline_execution = pipeline.start(\n",
    "#     parameters=dict(\n",
    "#         SkipDataQualityCheck=False,\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d527786-0c93-4592-8d91-4f2c95ec691b",
   "metadata": {},
   "source": [
    "### Additional features of SageMaker Pipelines\n",
    "Feel free to explore more useful features of SageMaker Pipelines on your own, such as [selective execution of pipeline steps](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-selective-ex.html), [cross-account support](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-xaccount.html), [scheduled pipeline runs](https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html), or [local mode](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-local-mode.html).\n",
    "\n",
    "If you need to add more monitoring functionality to the pipeline, you can use the [`MonitorBatchTransformStep`](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.monitor_batch_transform_step.MonitorBatchTransformStep) to combine a transform step with quality checks, bias detection, and explainability report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4252f74-aeea-488b-8628-9dacd566384b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e4252-719c-4cde-b88f-6eb1b729a955",
   "metadata": {},
   "source": [
    "## Optional: add a feature store\n",
    "In this section you're going to use SageMaker Feature Store to manage features and the dataset for model training. \n",
    "\n",
    "<div class=\"alert alert-info\"> This section is optional and not required for course of the workshop. You can stop here and go to the next notebook.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d68f4-4bd2-44e7-8dbd-742e05152760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.inputs import TableFormatEnum\n",
    "from sagemaker.feature_store.feature_processor import CSVDataSource, feature_processor, to_pipeline\n",
    "from sagemaker.remote_function import remote\n",
    "from sagemaker.workflow.function_step import step\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timezone, date\n",
    "from time import gmtime, strftime, sleep\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe3378-740f-4e46-98fd-490ef17d1834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a84977-e63c-4fd6-a643-2bc2d039229b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "project = \"from-idea-to-prod\"\n",
    "current_timestamp = strftime('%d-%H-%M-%S', gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62fd49-01ac-41bb-b3ea-6b640e509188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_store_bucket_prefix = 'from-idea-to-prod/feature-store'\n",
    "%store feature_store_bucket_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e101dd-d211-4389-9c8d-61f9513110fe",
   "metadata": {},
   "source": [
    "### Transform raw data into training-ready features\n",
    "First transform raw data into features in order to be able to extract the schema from the dataset. You need the data schema for definition of a feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c8373-3b55-4c75-94d5-7b48f50ac56e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load original raw data\n",
    "df_data = pd.read_csv(dataset_file_local_path, sep=\";\")\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2887120-c164-4b78-90c5-ede89722b9f5",
   "metadata": {},
   "source": [
    "Apply feature engineering to the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39290b0-12ef-4c96-9c76-cfb68671e8b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_col = \"y\"\n",
    "\n",
    "# Indicator variable to capture when pdays takes a value of 999\n",
    "df_data[\"no_previous_contact\"] = np.where(df_data[\"pdays\"] == 999, 1, 0)\n",
    "\n",
    "# Indicator for individuals not actively employed\n",
    "df_data[\"not_working\"] = np.where(\n",
    "    np.in1d(df_data[\"job\"], [\"student\", \"retired\", \"unemployed\"]), 1, 0\n",
    ")\n",
    "\n",
    "# remove unnecessary data\n",
    "df_model_data = df_data.drop(\n",
    "    [\"duration\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "bins = [18, 30, 40, 50, 60, 70, 90]\n",
    "labels = ['18-29', '30-39', '40-49', '50-59', '60-69', '70-plus']\n",
    "\n",
    "df_model_data['age_range'] = pd.cut(df_model_data.age, bins, labels=labels, include_lowest=True)\n",
    "df_model_data = pd.concat([df_model_data, pd.get_dummies(df_model_data['age_range'], prefix='age', dtype=int)], axis=1)\n",
    "df_model_data.drop('age', axis=1, inplace=True)\n",
    "df_model_data.drop('age_range', axis=1, inplace=True)\n",
    "\n",
    "scaled_features = ['pdays', 'previous', 'campaign']\n",
    "df_model_data[scaled_features] = MinMaxScaler().fit_transform(df_model_data[scaled_features])\n",
    "\n",
    "df_model_data = pd.get_dummies(df_model_data, dtype=int)  # Convert categorical variables to sets of indicators\n",
    "\n",
    "# Replace \"y_no\" and \"y_yes\" with a single label column, and bring it to the front:\n",
    "df_model_data = pd.concat(\n",
    "    [\n",
    "        df_model_data[\"y_yes\"].rename(target_col),\n",
    "        df_model_data.drop([\"y_no\", \"y_yes\"], axis=1),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed45a86-8f00-4e1b-a2c7-acec0fe622e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_event_timestamp():\n",
    "    # naive datetime representing local time\n",
    "    naive_dt = datetime.now()\n",
    "    # take timezone into account\n",
    "    aware_dt = naive_dt.astimezone()\n",
    "    # time in UTC\n",
    "    utc_dt = aware_dt.astimezone(timezone.utc)\n",
    "    # transform to ISO-8601 format\n",
    "    event_time = utc_dt.isoformat(timespec='milliseconds')\n",
    "    event_time = event_time.replace('+00:00', 'Z')\n",
    "    return event_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d29a2-8b82-4961-bbf0-4b655abb8101",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "Add `event_time` and `record_id` columns to the dataset as these two fields are required for each feature group:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22a358-de8e-4ac5-b036-54f35ec20cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_model_data['event_time'] = generate_event_timestamp()\n",
    "df_model_data['record_id'] = [f'R{i}' for i in range(len(df_model_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e9181d-aa22-42af-b9fd-d057d8b24cd1",
   "metadata": {},
   "source": [
    "Feature names cannot contain '.' and cannot end on '_'. Also remove '-' from the column names when converting column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b4220-e3e6-4abe-a193-5b9bb5030a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_col_name(c):\n",
    "    return c.replace('.', '_').replace('-', '_').rstrip('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d233973-f8d3-488f-9f92-44cbb5f34382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_model_data = df_model_data.rename(columns=convert_col_name)\n",
    "df_model_data = df_model_data.convert_dtypes(infer_objects=True, convert_boolean=False)\n",
    "df_model_data['record_id'] = df_model_data['record_id'].astype('string')\n",
    "df_model_data['event_time'] = df_model_data['event_time'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef70777-a343-487a-b087-b220734acbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_model_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e8afb-9761-4624-b4eb-72b9d5a6be3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_model_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388370e-4596-4968-8358-c26d9a44d03a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd4de7-8d8f-4b1d-b16b-671a33cedfd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_model_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602ddca-cb2b-4381-bac5-ac653a2adac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_model_data.to_csv('./data/feature_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5dddb9-467c-4ff3-8903-c782ae0a94ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_count = len(df_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5fbc1-3ee0-4e76-a3f1-1e3fdf9aed85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15454a42-b7f1-4bf8-8fb5-71358819cf23",
   "metadata": {},
   "source": [
    "### Create a feature group\n",
    "Now is everything ready to create a feature group with the dataset schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900eb526-ea42-4a09-85c4-883c6d6d6ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_feature_group_name = f'{project}-{current_timestamp}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75080b-c1c8-477c-8b07-76350de12ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store dataset_feature_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d46f8-f7dd-4799-b66d-c08f82b4b2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_feature_group = FeatureGroup(name=dataset_feature_group_name, sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b09632-0ca0-4186-94f8-04d373926030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the DataFrame to extract the feature group definitions\n",
    "dataset_feature_group.load_feature_definitions(data_frame=df_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b4a61-4605-4618-a471-2c46298aa9b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    print(f'Initial status: {status}')\n",
    "    while status == 'Creating':\n",
    "        print(f'Waiting for feature group: {feature_group.name} to be created ...')\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    if status != 'Created':\n",
    "        raise SystemExit(f'Failed to create feature group {feature_group.name}: {status}')\n",
    "    print(f'FeatureGroup {feature_group.name} was successfully created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573280f8-6a7f-430e-a037-162db0d80694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_feature_group.create(\n",
    "    s3_uri=f's3://{bucket_name}/{feature_store_bucket_prefix}', \n",
    "    record_identifier_name='record_id', \n",
    "    event_time_feature_name='event_time', \n",
    "    role_arn=sm_role, \n",
    "    enable_online_store=False,\n",
    "    table_format=TableFormatEnum.ICEBERG \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc59c18-db14-4563-9578-a1bec1161039",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "Wait until the feature group is created and ready for use. It takes less then a minute.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4ac82-e982-4f74-917c-25b78292a587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wait_for_feature_group_creation_complete(dataset_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997639b9-26bb-40a0-bf92-9d38e344932e",
   "metadata": {},
   "source": [
    "If you run this workshop in your own account or in any account not provisioned via an AWS-led event, you might have issues with creation of a feature group because of permission setup. Follow this [feature group creation troubleshooting guide](https://repost.aws/knowledge-center/sagemaker-featuregroup-troubleshooting). Permission issues are most often connected to missing LakeFormation permissions for the SageMaker execution role. You need to grant access rights to the database `sagemaker_featurestore` to the SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64184cff-44ed-49a4-8364-ffc755e6b01c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_feature_group.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a383fb27-7132-4439-bab4-b805be36d3c7",
   "metadata": {},
   "source": [
    "The feature group is ready for use. Now you need to ingest data into it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26346cd4-47fe-4129-ac28-de16a30175fb",
   "metadata": {},
   "source": [
    "### Ingest data into the feature group via a SageMaker pipeline\n",
    "Same as in the previous section you're going to use [`@step`](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-step-decorator-create-pipeline.html) decorator to create a feature ingestion pipeline.\n",
    "\n",
    "Compile all previous feature transformation and ingestion code into a remote function. The function code is in the file `ingest.py` in the `.\\pipeline_steps` folder. It uses the Python SDK [`FeatureGroup.ingest()`](https://sagemaker.readthedocs.io/en/stable/api/prep_data/feature_store.html#sagemaker.feature_store.feature_group.FeatureGroup.ingest) method to ingest the content of a pandas DataFrame to a feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ec025-5b17-40c7-a967-3ae9cd6a7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function code is in the local files\n",
    "from pipeline_steps.ingest import process_and_ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b8399-8426-4a23-8740-373c029f3bde",
   "metadata": {},
   "source": [
    "First run feature store ingestion locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b1cff-184f-4f98-bc71-959389e6fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_ingest(input_s3_url, dataset_feature_group.describe()['FeatureGroupArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d653c-3202-49ca-bc84-774277120a28",
   "metadata": {},
   "source": [
    "Define an ingestion pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39ac38-52d6-4b92-b26b-f8d5e0491000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create parameters for the feature store ingestion pipeline\n",
    "input_s3_url_param = ParameterString(\n",
    "    name=\"InputDataUrl\",\n",
    "    default_value=input_s3_url,\n",
    ")\n",
    "\n",
    "feature_group_name_param = ParameterString(\n",
    "    name=\"FeatureGroupName\",\n",
    "    default_value=dataset_feature_group.describe()['FeatureGroupArn'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518070fc-fdeb-42f2-9f22-586eeef57163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature store ingest step\n",
    "fs_ingest = step(\n",
    "    process_and_ingest, \n",
    "    name=f'{project}-fs-ingest',\n",
    ")(\n",
    "    input_s3_url=input_s3_url_param,\n",
    "    feature_group_name=feature_group_name_param,\n",
    ")\n",
    "\n",
    "# create a pipeline with an ingest step\n",
    "pipeline_fs_ingest = Pipeline(\n",
    "    name=f\"{pipeline_name}-fs-ingest\",\n",
    "    parameters=[\n",
    "        input_s3_url_param,\n",
    "        feature_group_name_param\n",
    "    ],\n",
    "    steps=[fs_ingest]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d6603-9308-4ed7-883d-ac86fb05255d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_fs_ingest.upsert(role_arn=sm_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704d0d7-a256-4319-b83c-0ea2679c126e",
   "metadata": {},
   "source": [
    "This feature ingestion pipeline consists of one step only. You can see the pipeline in Studio UX if you follow the like created by the next sell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad3a5d-80cf-4f37-bf88-b87e4e5a153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the pipeline link\n",
    "display(\n",
    "    HTML('<b>See <a target=\"top\" href=\"https://studio-{}.studio.{}.sagemaker.aws/pipelines/{}/graph\">the pipeline</a> in the Studio UI</b>'.format(\n",
    "            domain_id, region, pipeline_fs_ingest.name))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9e71b-2f80-4d81-800b-3fd122a6c9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execution_fs_ingest = pipeline_fs_ingest.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b16f27-5591-47b3-b7a9-11de8c829ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execution_fs_ingest.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d84bb8-c4b3-44a6-864d-785b099ccf63",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">You need to wait until the pipeline completes and ingested features into the feature group. The execution takes about 5 minuntes.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01456d7c-8b70-45ae-95fa-223347d238c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You need to wait until the feature ingestion pipeline execution completes\n",
    "execution_fs_ingest.wait()\n",
    "execution_fs_ingest.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676a276-4aaf-4ef2-a18f-1a44fdb8bb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert execution_fs_ingest.list_steps()[0]['StepStatus'] == 'Succeeded', 'Ingestion pipepline execution status must be Succeeded!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8202e-12e7-4933-bf8a-bbabeb584e18",
   "metadata": {},
   "source": [
    "### Feature Store in the Studio UI\n",
    "You can explore feature store and feature groups in the Studio UI. Navigate to **Data** > **Feature Store**:\n",
    "\n",
    "![](img/feature-store-studio-ui.png)\n",
    "\n",
    "In the UI you can explore features, feature group metadata, see sample queries, and associated pipeline executions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1cba0-e18d-4833-87de-dd24dfe95386",
   "metadata": {},
   "source": [
    "### Retrieve ingested features from the feature group\n",
    "\n",
    "There are many approaches to extract features from the offline feature store. For example, you can use [Amazon Athena query](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html#feature-store-athena-sample-queries) to query and join data stored in the offline store, or you can use [Offline Store Python SDK](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html#feature-store-dataset-python-sdk). You're going to use Python SDK to extract features and create a dataset for the model building pipeline.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <p style=\" text-align: center; margin: auto;\">Ingestion to the offline store is buffered and it takes up to 15 minutes for data to appear in the feature group. After features are ingested and available in the offline store, you can query them and create datasets for model training and scoring.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce438a-b1bf-47b4-82aa-5ebfb604244b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "output_location = f's3://{bucket_name}/{feature_store_bucket_prefix}/offline-store/query_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685accaa-3679-4e40-95b1-4263826e108d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_historical_record_count(fg):\n",
    "    fs_query = dataset_feature_group.athena_query()\n",
    "    query_string = f'SELECT COUNT(*) FROM \"' + fs_query.table_name + f'\"'\n",
    "    output_location =  f's3://{bucket_name}/{feature_store_bucket_prefix}/offline-store/query_results/'\n",
    "\n",
    "    fs_query.run(query_string=query_string, output_location=output_location)\n",
    "    fs_query.wait()\n",
    "    fs_df = fs_query.as_dataframe()\n",
    "    \n",
    "    return fs_df.iat[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f9b69-79bf-4056-bea4-afbb0ab88f77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">The next code cell waits until features appeared in the offline store. It might take up to 15 minutes. If you have already ingested features before, the cell will exit after the first query.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ef2b7-dbb8-4078-aa37-c0c46831a9db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before accessing the feature data you need to check if the offline feature store was populated\n",
    "offline_store_contents = None\n",
    "while offline_store_contents is None:    \n",
    "    fs_record_count = get_historical_record_count(dataset_feature_group)\n",
    "    print(f\"Total number of historical record in the {dataset_feature_group.name}: {fs_record_count}\")\n",
    "\n",
    "    if fs_record_count >= record_count:\n",
    "        print(f'[{fs_record_count} feature records are available in offline store for {dataset_feature_group.name} feature group]')\n",
    "        offline_store_contents = fs_record_count\n",
    "    else:\n",
    "        print('[Waiting for data arrives in offline store ...]')\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ad7d5-d1fc-4c6b-9dce-fdf8407b05ec",
   "metadata": {},
   "source": [
    "#### Use the Amazon SageMaker Python SDK (DatasetBuilder) to query the feature store\n",
    "This section demonstrates how to use [`DatasetBuilder`](https://sagemaker.readthedocs.io/en/stable/api/prep_data/feature_store.html#sagemaker.feature_store.dataset_builder.DatasetBuilder) to get data from feature groups. Refer to the [Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html) for detailed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0f0ab-1f41-40e8-9292-02d8a7137694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_store import FeatureStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f54a5-7d0a-4a62-875b-1f2a84723eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name=\"sagemaker\", region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name=\"sagemaker-featurestore-runtime\",region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad74743-b383-4706-bc6c-75b9004beb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create FeatureStore session object\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
    ")\n",
    "\n",
    "feature_store = FeatureStore(sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96e430-7370-41d6-acbb-8fd38acda684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "included_feature_names = [f.feature_name for f in dataset_feature_group.feature_definitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9feb8f-8c4d-4621-b558-7d634df694e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset builder to retrieve the most recent version of each record\n",
    "builder = feature_store.create_dataset(\n",
    "    base=dataset_feature_group,\n",
    "    # included_feature_names=included_feature_names,\n",
    "    output_path=output_location,\n",
    ").with_number_of_recent_records_by_record_identifier(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daad11c-4a6b-4c68-b2c4-95d04202ab9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dataset, query = builder.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dacb90a-0880-4ba3-9ddd-e0e779e05ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77c3d3-881c-47fe-ad90-ddd0eb6ba9e9",
   "metadata": {},
   "source": [
    "### Integrate a feature group in a model building pipeline\n",
    "So far you ingested all transformed features into the feature store. As the last step in this notebook you need to adapt the model building pipeline to use the transformed features from the feature group instead of loading and transforming a raw data file from an S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ff2c7-f979-42d6-ba8e-854ae6b5a24b",
   "metadata": {},
   "source": [
    "The code for feature exraction from the feature store and preparing datasets for training, test, validation, and quality baseline is in the Python file `extract.py` in the folder `./pipeline_steps`. \n",
    "Note, there is no feature processing code in the script because all feature engineering is done before ingesting features into feature store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0f3a6-1c61-43b6-8536-37824bc8cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function code is in the local files\n",
    "from pipeline_steps.extract import prepare_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc774c1-af6a-4693-b776-ac87e9dd35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function locally to verify everything works\n",
    "r_extract = prepare_datasets(\n",
    "    feature_group_name=dataset_feature_group_name,\n",
    "    output_s3_prefix=output_s3_prefix,\n",
    "    query_output_s3_path=output_location,\n",
    "    tracking_server_arn=mlflow_arn,\n",
    "    experiment_name=f\"local-test-{current_timestamp}\"\n",
    ")\n",
    "r_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdab255-972b-4c44-b777-1ff3fbfd91fb",
   "metadata": {},
   "source": [
    "You've just tested this script locally, now let's integrate it into the model building pipeline. The next cell contains the full script for pipeline building.\n",
    "\n",
    "For clarity, the batch monitoring and quality monitoring steps are omitted from this pipeline. Feel free to add these steps using the code from the section **Add a batch transform and quality checks to the pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366afee-1c2b-4244-9402-c5162c215dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_steps.evaluate import evaluate\n",
    "from pipeline_steps.register import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f495c8-af62-48e7-a01f-4934fe309ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = PipelineSession()\n",
    "experiment_name = f\"{project}-fs-pipeline-{current_timestamp}\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# extract features from the feature store step\n",
    "fs_step_extract_featureset = step(\n",
    "    prepare_datasets, \n",
    "    instance_type=process_instance_type_param,\n",
    "    name=f\"{project}-extract-featureset\",\n",
    ")(\n",
    "    feature_group_name=feature_group_name_param,\n",
    "    output_s3_prefix=output_s3_prefix,\n",
    "    query_output_s3_path=output_location,\n",
    "    tracking_server_arn=tracking_server_arn_param,\n",
    "    experiment_name=experiment_name,\n",
    "    pipeline_run_name=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    ")\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True)\n",
    "cache_config.expire_after = \"p30d\"\n",
    "\n",
    "# train step\n",
    "fs_step_train = TrainingStep(\n",
    "    name=f\"{project}-train\",\n",
    "    step_args=get_xgb_estimator(\n",
    "        session=session,\n",
    "        instance_type=train_instance_type_param,\n",
    "        output_s3_url=output_s3_url,\n",
    "        base_job_name=f\"{project}-train\",\n",
    "    ).fit(\n",
    "        {\n",
    "            \"train\": TrainingInput(\n",
    "                fs_step_extract_featureset['train_data'],\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                fs_step_extract_featureset['validation_data'],\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "        }\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")    \n",
    "\n",
    "# evaluate step\n",
    "fs_step_evaluate = step(\n",
    "    evaluate,\n",
    "    instance_type=process_instance_type_param,\n",
    "    name=f\"{project}-evaluate\",\n",
    ")(\n",
    "    test_x_data_s3_path=fs_step_extract_featureset['test_x_data'],\n",
    "    test_y_data_s3_path=fs_step_extract_featureset['test_y_data'],\n",
    "    model_s3_path=fs_step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    output_s3_prefix=output_s3_prefix,\n",
    "    tracking_server_arn=tracking_server_arn_param,\n",
    "    experiment_name=fs_step_extract_featureset['experiment_name'],\n",
    "    pipeline_run_id=fs_step_extract_featureset['pipeline_run_id'],\n",
    ")\n",
    "\n",
    "# register model step\n",
    "fs_step_register = step(\n",
    "        register,\n",
    "        name=f\"{project}-register\",\n",
    "    )(\n",
    "        training_job_name=fs_step_train.properties.TrainingJobName,\n",
    "        model_package_group_name=model_package_group_name_param,\n",
    "        model_approval_status=model_approval_status_param,\n",
    "        evaluation_result=fs_step_evaluate['evaluation_result'],\n",
    "        output_s3_prefix=output_s3_url,\n",
    "        tracking_server_arn=tracking_server_arn_param,\n",
    "        experiment_name=fs_step_extract_featureset['experiment_name'],\n",
    "        pipeline_run_id=fs_step_extract_featureset['pipeline_run_id'],\n",
    "    )\n",
    "\n",
    "# fail the pipeline execution step\n",
    "fs_step_fail = FailStep(\n",
    "    name=f\"{project}-fail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to AUC Score < \", test_score_threshold_param]),\n",
    ")\n",
    "\n",
    "# condition to check in the condition step\n",
    "fs_condition_gte = ConditionGreaterThanOrEqualTo(\n",
    "        left=fs_step_evaluate['evaluation_result']['classification_metrics']['auc_score']['value'],  \n",
    "        right=test_score_threshold_param,\n",
    ")\n",
    "\n",
    "# conditional register step\n",
    "fs_step_conditional_register = ConditionStep(\n",
    "    name=f\"{project}-check-metrics\",\n",
    "    conditions=[fs_condition_gte],\n",
    "    if_steps=[fs_step_register],\n",
    "    else_steps=[fs_step_fail],\n",
    ")\n",
    "\n",
    "# Create a pipeline object\n",
    "pipeline_feature_store = Pipeline(\n",
    "    name=f\"{pipeline_name}-fs\",\n",
    "    parameters=[\n",
    "        feature_group_name_param,\n",
    "        process_instance_type_param,\n",
    "        train_instance_type_param,\n",
    "        model_approval_status_param,\n",
    "        test_score_threshold_param,\n",
    "        model_package_group_name_param,\n",
    "        tracking_server_arn_param,\n",
    "    ],\n",
    "    steps=[fs_step_conditional_register],\n",
    "    pipeline_definition_config=PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38bf7c-f5a1-4b74-9b5b-7f461c8978bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_feature_store.upsert(role_arn=sm_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0825d1a-07cb-4727-95b6-898a61e723b5",
   "metadata": {},
   "source": [
    "The new pipeline looks exactly the same as the first pipeline but has a featureset extraction step instead of feature engineering - `from-idea-to-prod-extract-featureset`. To see the pipeline in the Studio UX click on the link constructed by the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e8abf-c5f7-4383-ab83-1197ee4f6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the pipeline link\n",
    "display(\n",
    "    HTML('<b>See <a target=\"top\" href=\"https://studio-{}.studio.{}.sagemaker.aws/pipelines/{}/graph\">the pipeline</a> in the Studio UI</b>'.format(\n",
    "            domain_id, region, pipeline_feature_store.name))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9d895-666e-4d01-8d87-1318fdafe2a9",
   "metadata": {},
   "source": [
    "### Execute a new pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e82aaa-72f7-4d79-8aac-2025b3e25f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_feature_store = pipeline_feature_store.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc5f4ad-736a-419a-88c0-9fde3356e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these two lines if you'd like to wait until execution is done\n",
    "# execution_feature_store.wait()\n",
    "# execution_feature_store.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da456c67-e0bc-441c-8f04-5e55bd1ac5d5",
   "metadata": {},
   "source": [
    "## Further reading: use Feature Processor for feature transformation and ingestion\n",
    "SageMaker provides you with a Spark-based [Feature Processor SDK](https://sagemaker.readthedocs.io/en/stable/api/prep_data/feature_store.html#feature-processor-decorator) with which you can transform and ingest data from batch data sources into your feature groups. Read through the description and examples in the [Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-feature-processing.html).\n",
    "\n",
    "Refer to a more detailed example of feature processor in [feature store feature processor](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-featurestore/feature_store_feature_processor.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bac953-1d16-47b4-939b-c8f9f2966c0e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook you've built three SageMaker pipelines:\n",
    "- An initial model building pipeline with data processing, model training, model evaluation, and conditional model registration steps\n",
    "- Next version of this pipeline with data and model quality monitoring and batch transform\n",
    "- A feature engineering and ingestion into the SageMaker Feature Store pipeline\n",
    "- An adapted version of the initial pipeline with the data processing step replaced by the featureset extraction step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749da96-9b82-465c-8328-eecd515d23a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1eb4e6-6010-49c3-ada7-9935bf8644b8",
   "metadata": {},
   "source": [
    "## Continue with the workshop flow\n",
    "After finishing this lab, you can continue with the step 4 and 5 [notebooks](04-sagemaker-project.ipynb) or go directly to the step 6 [notebook](06-monitoring.ipynb):\n",
    "\n",
    "- **Step 4 and 5 notebooks**: Use SageMaker Projects to implement CI/CD automation pipelines for model build and deployment\n",
    "\n",
    "- **Step 6 notebooks**: Data and model quality monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9dea1-ec92-4a28-8523-31c990c274a4",
   "metadata": {},
   "source": [
    "## Further development ideas for your real-world projects\n",
    "- Add [bias detection and model explainability](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-clarify-check) steps. Add model metrics calculated by [SageMaker Clarify](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html) to the model metadata in the model registry\n",
    "- Add event-driven launching of the ML pipeline as soon as a new dataset is uploaded to an Amazon S3 bucket. You can use [Amazon EventBridge integeration](https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html#pipeline-eventbridge-schedule) to implement various event-driven workflows\n",
    "- Use a designated IAM execution role for the pipeline execution\n",
    "- Add data encryption by using S3 bucket encryption and AWS KMS keys for container EBS volume encryption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f9efc-0920-4077-bd58-9f5514550412",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "- [Automate Machine Learning Workflows](https://aws.amazon.com/getting-started/hands-on/machine-learning-tutorial-mlops-automate-ml-workflows/)\n",
    "- [Amazon SageMaker Feature Store workshop](https://github.com/aws-samples/amazon-sagemaker-feature-store-end-to-end-workshop)\n",
    "- [Amazon SageMaker Model Building Pipeline](https://github.com/aws/sagemaker-python-sdk/blob/master/doc/amazon_sagemaker_model_building_pipeline.rst)\n",
    "- [MLOPs With SageMaker Pipelines Step Decorator](https://towardsaws.com/mlops-with-sagemaker-pipelines-step-decorator-bb63fce88846)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874abcd0-2241-4118-9191-011215b861cd",
   "metadata": {},
   "source": [
    "# Shutdown kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139bfea5-a835-4adc-81f0-e5355bf53a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10819548-ebf7-4cdf-8084-0f33a148c214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
